# -*- coding: utf-8 -*-
"""MINI_PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zzhm8N0xFaf4gmzM-FSGsKNT0OEe1PxR
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import BisectingKMeans

spark = SparkSession.builder.appName("Movie_recommendation").getOrCreate()

movies= spark.read.csv('Movies.csv', header=True, inferSchema=True)

tags = spark.read.csv("tags.csv", header=True, inferSchema=True)
print(list(tags.columns))

from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler, StandardScaler

genresIndexer = StringIndexer(inputCol="Genres", outputCol="genresIndex")
genresDF = genresIndexer.fit(movies).transform(movies)
time_scaler = StandardScaler(inputCol='Timestamp', outputCol='time_scaled')

tagIndexer = StringIndexer(inputCol="Tag", outputCol="tagIndex")
tagDF = tagIndexer.fit(tags).transform(tags)

from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

df=genresDF.join(tagDF,"MovieID")
df.show(2)

from pyspark.sql.functions import col
joinedDF = df.withColumn("float_time", col("Timestamp").cast("float"))

assembler = VectorAssembler(inputCols=['genresIndex', 'tagIndex', 'float_time'], outputCol='features')

vectors=assembler.setHandleInvalid("skip").transform(joinedDF)

from pyspark.ml.feature import BucketedRandomProjectionLSH

lsh = BucketedRandomProjectionLSH(inputCol='features', outputCol='hashes', bucketLength=2.0, numHashTables=3)
model = lsh.fit(vectors)
similarity = model.approxSimilarityJoin(vectors, vectors, threshold=0.8, distCol='cosine_similarity')



from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import matplotlib.pyplot as plt

# Create the KMeans model
kmeans = KMeans(featuresCol="features")

# Define the range of cluster numbers to evaluate
min_clusters = 2
max_clusters = 14
cluster_range = range(min_clusters, max_clusters + 1)

# Initialize lists to store the cluster numbers and DBI values
cluster_numbers = []
dbi_values = []

# Perform clustering and calculate DBI for each cluster number
for num_clusters in cluster_range:
    # Set the number of clusters for the KMeans model
    kmeans.setK(num_clusters)

    # Fit the model on the data
    model = kmeans.fit(vectors)

    # Make predictions
    predictions = model.transform(vectors)

    # Evaluate clustering using the ClusteringEvaluator
    evaluator = ClusteringEvaluator()
    dbi = evaluator.evaluate(predictions)

    # Store the cluster number and DBI value
    cluster_numbers.append(num_clusters)
    dbi_values.append(dbi)

# Plot the DBI values against the number of clusters
plt.plot(cluster_numbers, dbi_values, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('DB Index')
plt.title('Davies-Bouldin Index vs. Number of Clusters')
plt.show()

#evaluator = ClusteringEvaluator(featuresCol='features', predictionCol='prediction', metricName='silhouette')
#scores = []
#for k in range(2, 15):
#    kmeans = KMeans(featuresCol='features', k=k, seed=1)
 #   model = kmeans.fit(vectors)
  #  predictions = model.transform(vectors)
   # score = evaluator.evaluate(predictions)
    #scores.append(score)
#optimal_k = scores.index(min(scores)) + 2

#kmeans = KMeans(featuresCol='features', k=optimal_k, seed=1)
#model = kmeans.fit(vectors)
#predictions = model.transform(vectors)
#db_index = evaluator.evaluate(predictions, {evaluator.metricName: 'silhouette'})
#print(db_index)

#import matplotlib.pyplot as plt

#plt.plot(range(2, 15), scores)
#plt.xlabel('Number of clusters')
#plt.ylabel('Davies-Bouldin index')
#plt.title('DB index vs Number of clusters')
#plt.show()

from pyspark.ml.evaluation import ClusteringEvaluator
import matplotlib.pyplot as plt

bkm = BisectingKMeans(featuresCol='features', k=7, seed=1)
model = bkm.fit(vectors)

predictions = model.transform(vectors)

features = predictions.select('features').rdd.map(lambda x: x[0]).collect()
labels = predictions.select('prediction').rdd.map(lambda x: x[0]).collect()
plt.scatter([x[0] for x in features], [x[1] for x in features], c=labels)
plt.xlabel('Genre')
plt.ylabel('Tag')
plt.title('Bisecting KMeans Clustering')
plt.show()

predictions.show(10)

from pyspark.sql.functions import split, explode

# split the Genres column by "|" and explode it into multiple rows
predictions = predictions.withColumn("genre", explode(split("Genres", "\|")))

# group by the prediction and genre columns to count the frequency of each genre within each cluster
counts = predictions.groupby("prediction", "genre").count()

# use window functions to get the genre with the highest frequency within each cluster
from pyspark.sql.window import Window
from pyspark.sql.functions import rank

window = Window.partitionBy("prediction").orderBy(counts["count"].desc())
top_genre = counts.select("*", rank().over(window).alias("rank")).filter("rank == 1")

# join the top_genre dataframe with the original dataframe to get the cluster names
cluster_names = predictions.join(top_genre, ["prediction", "genre"]).select("prediction", "genre", "count")

predictions.show()

from pyspark.sql.functions import desc
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from pyspark.sql.functions import avg

# calculate the average timestamp for each cluster
avg_timestamp = predictions.groupBy('prediction').agg(avg('timestamp').alias('avg_timestamp'))

# join the average timestamp with the original dataframe
joined_df = predictions.join(avg_timestamp, on='prediction')

# calculate the time difference for each item
joined_df = joined_df.withColumn('time_diff', col('avg_timestamp') - col('timestamp'))

# calculate the average time difference for each cluster
avg_time_diff = joined_df.groupBy('prediction').agg(avg('time_diff').alias('avg_time_diff'))

# calculate the threshold value based on the average time difference
threshold = avg_time_diff.select(avg('avg_time_diff')).collect()[0][0]

# categorize the users into recent, medium, and old categories
from pyspark.sql.functions import when
joined_df = joined_df.withColumn('user_category',
                                  when(col('time_diff') < threshold, 'Old')
                                  .when(col('time_diff') > threshold , 'Recent')
                                  .otherwise('Medium'))

# define the window specification
from pyspark.sql.functions import avg, col, when, rank
from pyspark.sql.window import Window

rank_window = Window.partitionBy('prediction', 'user_category').orderBy(col('timestamp').desc())

# rank the items within each category based on the timestamp
joined_df = joined_df.withColumn('rank', rank().over(rank_window))

# show 10 random rows from joined_df
joined_df.show(5)

#joined_df.sample(fraction=0.3).show(10)

rating= spark.read.csv('ratings.csv', header=True, inferSchema=True)

rating=rating.drop("MovieID")

rating.join(joined_df,"UserID")

from pyspark.sql.functions import max, when, col

# calculate the maximum rating for each user
max_rating_df = rating.groupBy('UserID').agg(max('Rating').alias('max_rating'))

# join the maximum rating with the joined_df
data_df = rating.join(max_rating_df, ['UserID'])
data_df.show(2)

rating_df = data_df.withColumn('preference_level', col('rating')/col('max_rating'))

# categorize the preference level into high, low, and medium
rating_df = rating_df.withColumn('preference_category',
                                 when(col('preference_level') > 0.7, 'high')
                                 .when(col('preference_level') < 0.3, 'low')
                                 .otherwise('medium'))

rating_df.show(10)

from pyspark.sql.functions import col

final_df=rating_df.join(joined_df,"UserID")

# show 10 random rows from joined_df
final=final_df.sample(fraction=0.1)
final.show()

from pyspark.sql.functions import concat, col, lit

# Assuming the DataFrame is named "df"
df_concat = final.withColumn("combined_category",
                   concat(col("user_category"), lit(", "),
                          col("preference_category"), lit(", "),
                          col("genre")))

new_df=df_concat.select("UserID","MovieID", "Title","user_category", "preference_category", "genre","combined_category")
new_df.show(5)

# Split the association rules into training and testing datasets
train_ratio = 0.8
test_ratio = 1 - train_ratio
seed = 12345
train_data, test_data = new_df.randomSplit([train_ratio, test_ratio], seed=seed)

from pyspark.sql.functions import col, collect_set, concat_ws
from pyspark.ml.fpm import FPGrowth

# Sample a fraction of the data
fraction = 0.2
sampled_df = train_data.sample(fraction=fraction)

# Group the data by UserID and collect the combined categories in a set for each user
user_categories = sampled_df.groupBy("UserID").agg(collect_set("combined_category").alias("categories"))

# Run FPGrowth on the sampled data
fpGrowth = FPGrowth(itemsCol="categories", minSupport=0.1, minConfidence=0.5)
model = fpGrowth.fit(user_categories)
association_rules = model.associationRules

# Add aliases to the generated columns

association_rules = association_rules.withColumnRenamed("UserID", "userId")
association_rules = association_rules.withColumnRenamed("antecedent", "antecedent_itemset")
association_rules = association_rules.withColumnRenamed("consequent", "consequent_itemset")
association_rules = association_rules.withColumnRenamed("confidence", "confidence_level")

# Show the association rules
association_rules.show()

import matplotlib.pyplot as plt
import time

# Initialize empty lists for support and execution time
supports = []
execution_times = []

# Loop through support values and record execution time for each
for support in range(1, 10):
    support /= 10.0  # Convert to float
    start_time = time.time()
    fpGrowth = FPGrowth(itemsCol="categories", minSupport=support, minConfidence=0.5)
    model = fpGrowth.fit(user_categories)
    end_time = time.time()
    execution_time = end_time - start_time
    supports.append(support)
    execution_times.append(execution_time)

# Create a line plot
plt.plot(supports, execution_times)

# Customize plot
plt.title('Support vs Execution Time')
plt.xlabel('Support Level')
plt.ylabel('Execution Time (seconds)')

# Display plot
plt.show()

import matplotlib.pyplot as plt

# count the number of rules at each confidence level
num_rules_by_confidence = association_rules.groupBy("confidence_level").count().orderBy("confidence_level")

# extract the confidence levels and counts as arrays
confidences = num_rules_by_confidence.select("confidence_level").rdd.flatMap(lambda x: x).collect()
num_rules = num_rules_by_confidence.select("count").rdd.flatMap(lambda x: x).collect()

# plot the results
plt.plot(confidences, num_rules)
plt.xlabel("confidence level")
plt.ylabel("number of rules")
plt.show()

# Select strong association rules based on confidence level, lift value, and support count
min_confidence = 0.8
min_lift = 1.2
min_support = 0.2

strong_rules = association_rules.filter((col("confidence_level") >= min_confidence) &
                                         (col("lift") >= min_lift) &
                                         (col("support") >= min_support))

# Sort the rules by decreasing confidence level
#sorted_rules = strong_rules.sort(col("confidence_level").desc())

# Show the top 10 rules
strong_rules.show(10)

# Select the top n rules based on confidence level
n = 10
# Get the top itemsets from the association rules
top_rules = association_rules.sort(col("confidence_level").desc()).limit(10)
#top_itemsets = set(top_rules.select(col("antecedent_itemset")).rdd.flatMap(lambda x: x).map(tuple).collect() + top_rules.select(col("consequent_itemset")).rdd.flatMap(lambda x: x).map(tuple).collect())
top_itemsets = set([tuple(itemset) for itemset in top_rules.select(col("antecedent_itemset")).rdd.flatMap(lambda x: x).collect() + top_rules.select(col("consequent_itemset")).rdd.flatMap(lambda x: x).collect()])


top_rules.show()
#training_data = train_data.filter(col("combined_category").isin(top_itemsets))

print("Top itemsets:")
for itemset in top_itemsets:
    print(itemset)

from pyspark.sql.types import StructType, StructField, StringType
from pyspark.ml.feature import StringIndexer
from pyspark.sql.types import DoubleType
from pyspark.ml.classification import RandomForestClassifier


from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.sql.functions import col, udf
from pyspark.sql.types import DoubleType, StructType, StructField, StringType

from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, OneHotEncoderModel
from pyspark.sql.functions import when, col

def build_classifier_model(association_rules, train_data):
    # Convert the association rules into a dictionary for efficient lookup
    rules_dict = association_rules.rdd.filter(lambda x: x.confidence_level is not None).map(lambda x: ((frozenset(x.antecedent_itemset), frozenset(x.consequent_itemset)), x.confidence_level)).collectAsMap()

    # Define a UDF for extracting the antecedent features from the combined category
    extract_antecedent_udf = udf(lambda x: x[:-1])

    # Extract the antecedent features from the combined category
    train_data = train_data.withColumn("antecedent_features", extract_antecedent_udf(col("combined_category")))

    # Create a list of all the unique antecedent feature combinations
    antecedent_list = train_data.select("antecedent_features").distinct().rdd.flatMap(lambda x: x).collect()

    # Create a new DataFrame with one row for each unique antecedent feature combination
    schema = StructType([StructField("features", StringType(), True)])
    antecedent_df = spark.createDataFrame([(antecedent,) for antecedent in antecedent_list], schema=schema)

    # Define a StringIndexer to convert the antecedent features to numeric values
    indexer = StringIndexer(inputCol="features", outputCol="indexed_features")

    # Fit the indexer on the antecedent features
    antecedent_indexer_model = indexer.fit(antecedent_df)

    # Define a OneHotEncoder to convert the indexed features to binary vectors
    antecedent_encoder = OneHotEncoder(inputCols=["indexed_features"], outputCols=["antecedent_vector"])

    # Fit the encoder on the indexed features
    antecedent_encoder_model = antecedent_encoder.fit(antecedent_indexer_model.transform(antecedent_df))

    # Use the fitted encoder to transform the indexed features
    antecedent_encoded = antecedent_encoder_model.transform(antecedent_indexer_model.transform(antecedent_df))

     # Join the encoded antecedent features with the training data
    train_data = train_data.join(antecedent_encoded, train_data.antecedent_features == antecedent_encoded.features, "left").drop("features")
    train_data = train_data.join(final, ["UserID", "MovieID"], "left_outer").fillna(0)

    # Make a prediction for each antecedent feature combination using the association rules
    predict_udf = udf(lambda x: max([rules_dict.get((frozenset(antecedent.split(",")), frozenset(x.split(","))), 0.0) for antecedent in antecedent_list]), DoubleType())
    train_data = train_data.withColumn("predicted_confidence", predict_udf(col("antecedent_features")))

    # Convert the predicted confidence to a vector
    assembler = VectorAssembler(inputCols=["predicted_confidence"], outputCol="features_1")
    train_data = assembler.transform(train_data)
    # Define the target variable
    target_col = "has_rated_movie"
    # Create a new column that contains the class labels
    train_data = train_data.withColumn(target_col, when(col("rating") > 0, 1).otherwise(0))
    print(train_data)

    return model, antecedent_list

from pyspark.sql.types import ArrayType, DoubleType
from pyspark.sql.functions import array
import pyspark
from pyspark.sql import SparkSession

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("sample").getOrCreate()

model, antecedent_list = build_classifier_model(association_rules, train_data)

def test_classifier_model(model, association_rules, test_data, n_recommendations, antecedent_list):
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import udf, col
    from pyspark.sql.types import DoubleType, ArrayType

    try:
        # Try to get an existing SparkSession object
        spark = SparkSession.builder.appName("test_classifier_model2").getOrCreate()
    except:
        # If no existing SparkSession object is found, create a new one
        spark = SparkSession.builder.appName("test_classifier_model2").getOrCreate()

    rules_dict = association_rules.rdd.map(lambda x: ((tuple(x.antecedent_itemset), tuple(x.consequent_itemset)), x.confidence_level)).collectAsMap()
    extract_antecedent_udf = udf(lambda x: x[:-1])
    test_data = test_data.withColumn("antecedent_features", extract_antecedent_udf(col("combined_category")))
    predict_udf = udf(lambda x: [rules_dict.get((frozenset(antecedent.split(",")), frozenset(x.split(","))), 0.0) for antecedent in antecedent_list], ArrayType(DoubleType()))
    predictions = test_data.withColumn("probability", predict_udf(col("antecedent_features")))
    sorted_predictions = predictions.orderBy(col("probability").desc())

    return sorted_predictions

n_recommendations = 10
result = test_classifier_model(model, association_rules, test_data, n_recommendations, antecedent_list)
print(result)
result = result.drop(col('probability'))
result = result.drop(col('MovieID'))


result.show(10)

